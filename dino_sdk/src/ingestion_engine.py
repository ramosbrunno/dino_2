#!/usr/bin/env python3
"""
Dino SDK - Ingestion Engine
Motor de ingest√£o batch para Databricks com Unity Catalog
"""

import os
import time
from datetime import datetime
from typing import Optional, Dict, Any, List
from pathlib import Path


class IngestionEngine:
    """
    Motor de ingest√£o batch para Databricks
    
    Funcionalidades:
    - Ingest√£o batch de arquivos (CSV, JSON, Parquet, Delta, Avro)
    - Integra√ß√£o com Unity Catalog
    - Detec√ß√£o autom√°tica de formato
    - Metadados de auditoria
    - Valida√ß√£o de pr√©-requisitos
    
    Pr√©-requisitos:
    - Schema de destino deve existir
    - Permiss√µes adequadas no Unity Catalog
    """
    
    def __init__(
        self,
        target_schema: str,
        table_name: str,
        file_path: str,
        delimiter: str = ",",
        catalog_name: Optional[str] = None,
        output_mode: str = "append",
        file_format: Optional[str] = None,
        checkpoint_location: Optional[str] = None
    ):
        """
        Inicializa o motor de ingest√£o
        
        Args:
            target_schema: Schema de destino (deve existir previamente)
            table_name: Nome da tabela de destino
            file_path: Caminho do arquivo ou diret√≥rio de origem
            delimiter: Delimitador para arquivos CSV (padr√£o: ",")
            catalog_name: Nome do cat√°logo Unity Catalog
            output_mode: Modo de escrita (append, overwrite, merge)
            file_format: Formato do arquivo (detectado automaticamente se None)
            checkpoint_location: Localiza√ß√£o do checkpoint para streaming
        """
        self.target_schema = target_schema
        self.table_name = table_name
        self.file_path = file_path
        self.delimiter = delimiter
        self.catalog_name = catalog_name or self._get_default_catalog()
        self.output_mode = output_mode
        self.file_format = file_format
        self.checkpoint_location = checkpoint_location
        
        # Detectar formato se n√£o fornecido
        if not self.file_format:
            self.file_format = self._detect_file_format()
        
        # Gerar checkpoint location se n√£o fornecido (para streaming)
        if not self.checkpoint_location:
            self.checkpoint_location = f"/tmp/checkpoints/{self.target_schema}/{self.table_name}"
        
        # Valida√ß√µes
        self._validate_parameters()
    
    def _validate_parameters(self):
        """Valida os par√¢metros de entrada"""
        if not self.target_schema or not self.table_name:
            raise ValueError("target_schema e table_name s√£o obrigat√≥rios")
        
        if not self.file_path:
            raise ValueError("file_path √© obrigat√≥rio")
        
        # Validar formato suportado
        supported_formats = ["csv", "json", "parquet", "delta", "avro"]
        if self.file_format not in supported_formats:
            raise ValueError(f"Formato {self.file_format} n√£o suportado. Use: {supported_formats}")
        
        # Validar modo de sa√≠da
        valid_modes = ["append", "overwrite", "merge"]
        if self.output_mode not in valid_modes:
            raise ValueError(f"output_mode deve ser um de: {valid_modes}")
    
    def _get_default_catalog(self) -> str:
        """Obt√©m o cat√°logo padr√£o do workspace"""
        return os.getenv("DINO_DEFAULT_CATALOG", "main")
    
    def _detect_file_format(self) -> str:
        """Detecta o formato do arquivo baseado na extens√£o"""
        if self.file_path.endswith('/'):
            # Diret√≥rio - assumir primeiro arquivo encontrado
            return "csv"  # Padr√£o para diret√≥rios
        
        extension = Path(self.file_path).suffix.lower()
        format_map = {
            '.csv': 'csv',
            '.json': 'json', 
            '.jsonl': 'json',
            '.parquet': 'parquet',
            '.delta': 'delta',
            '.avro': 'avro'
        }
        
        detected = format_map.get(extension, 'csv')
        print(f"üìã Formato detectado: {detected}")
        return detected
    
    def get_table_full_name(self) -> str:
        """Retorna o nome completo da tabela"""
        return f"{self.catalog_name}.{self.target_schema}.{self.table_name}"
    
    def _check_schema_exists(self) -> bool:
        """Verifica se o schema existe (simula√ß√£o para ambiente local)"""
        # Em ambiente Databricks real, isso seria:
        # spark.sql(f"SHOW SCHEMAS IN {self.catalog_name}").filter(col("schemaName") == self.target_schema).count() > 0
        
        print(f"üîç Verificando se schema {self.catalog_name}.{self.target_schema} existe...")
        print(f"‚úÖ Schema validado (simula√ß√£o - em Databricks seria verifica√ß√£o real)")
        return True
    
    def _generate_streaming_code(self) -> str:
        """Gera c√≥digo PySpark para ingest√£o streaming com Auto Loader"""
        table_full_name = self.get_table_full_name()
        
        code = f'''
# Dino SDK - Ingest√£o Streaming com Auto Loader
# Gerado automaticamente em {datetime.now().isoformat()}

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from delta.tables import DeltaTable
import time

# Configura√ß√µes da ingest√£o
SOURCE_PATH = "{self.file_path}"
TARGET_TABLE = "{table_full_name}"
CHECKPOINT_LOCATION = "{self.checkpoint_location}"
FILE_FORMAT = "{self.file_format}"
DELIMITER = "{self.delimiter}"

print(f"üöÄ Iniciando ingest√£o streaming com Auto Loader")
print(f"üìÅ Origem: {{SOURCE_PATH}}")
print(f"üìä Destino: {{TARGET_TABLE}}")
print(f"üíæ Checkpoint: {{CHECKPOINT_LOCATION}}")
print(f"üìã Formato: {{FILE_FORMAT}}")

# Configurar Auto Loader
auto_loader_options = {{
    "cloudFiles.format": FILE_FORMAT,
    "cloudFiles.schemaLocation": f"{{CHECKPOINT_LOCATION}}/schema",
    "cloudFiles.useNotifications": "true",
    "cloudFiles.includeExistingFiles": "false",
    "cloudFiles.maxFilesPerTrigger": "100"
}}

# Adicionar op√ß√µes espec√≠ficas para CSV
if FILE_FORMAT == "csv":
    auto_loader_options.update({{
        "cloudFiles.delimiter": DELIMITER,
        "cloudFiles.header": "true",
        "cloudFiles.inferSchema": "true"
    }})

# Configurar stream de leitura
print("üìñ Configurando Auto Loader...")
df_stream = (spark.readStream
    .format("cloudFiles")
    .options(**auto_loader_options)
    .load(SOURCE_PATH))

# Adicionar metadados de processamento
df_with_metadata = (df_stream
    .withColumn("_dino_ingestion_timestamp", current_timestamp())
    .withColumn("_dino_source_file", input_file_name())
    .withColumn("_dino_file_modification_time", 
               col("_metadata.file_modification_time"))
    .withColumn("_dino_batch_id", expr("uuid()"))
    .withColumn("_dino_ingestion_mode", lit("streaming"))
    .withColumn("_dino_table_name", lit("{self.table_name}"))
    .withColumn("_dino_schema_name", lit("{self.target_schema}"))
)

print("‚úÖ Stream configurado com metadados de auditoria")

# Fun√ß√£o para processar batch
def process_batch(batch_df, batch_id):
    print(f"üì¶ Processando batch {{batch_id}}")
    print(f"üìä Registros no batch: {{batch_df.count()}}")
    
    # Salvar na tabela de destino
    (batch_df.write
        .format("delta")
        .mode("{self.output_mode}")
        .option("mergeSchema", "true")
        .saveAsTable(TARGET_TABLE))
    
    print(f"‚úÖ Batch {{batch_id}} processado com sucesso")

# Configurar streaming query
print("‚ö° Iniciando streaming query...")
query = (df_with_metadata.writeStream
    .foreachBatch(process_batch)
    .option("checkpointLocation", CHECKPOINT_LOCATION)
    .trigger(availableNow=False)  # Modo cont√≠nuo
    .start())

print("üîÑ Streaming em execu√ß√£o. Monitore os logs para acompanhar o progresso.")
print("‚èπÔ∏è Para parar, execute: query.stop()")

# Aguardar (opcional - remova se quiser que rode em background)
# query.awaitTermination()
'''
        return code
    
    def _generate_batch_code(self) -> str:
        """Gera c√≥digo PySpark para ingest√£o batch"""
        table_full_name = self.get_table_full_name()
        
        # Configura√ß√µes de leitura baseadas no formato
        read_options = self._get_read_options()
        
        code = f'''
# Dino SDK - Ingest√£o Batch
# Gerado automaticamente em {datetime.now().isoformat()}

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from delta.tables import DeltaTable
import time

# Configura√ß√µes da ingest√£o
SOURCE_PATH = "{self.file_path}"
TARGET_TABLE = "{table_full_name}"
FILE_FORMAT = "{self.file_format}"
OUTPUT_MODE = "{self.output_mode}"

print(f"üöÄ Iniciando ingest√£o batch")
print(f"üìÅ Origem: {{SOURCE_PATH}}")
print(f"üìä Destino: {{TARGET_TABLE}}")
print(f"üìã Formato: {{FILE_FORMAT}}")
print(f"üíæ Modo: {{OUTPUT_MODE}}")

# Configurar leitura baseada no formato
print("üìñ Configurando leitura de dados...")

{self._generate_read_code()}

# Adicionar metadados de auditoria
print("üè∑Ô∏è Adicionando metadados de auditoria...")
df_with_metadata = (df_source
    .withColumn("_dino_ingestion_timestamp", current_timestamp())
    .withColumn("_dino_source_path", lit(SOURCE_PATH))
    .withColumn("_dino_batch_id", expr("uuid()"))
    .withColumn("_dino_ingestion_mode", lit(OUTPUT_MODE))
    .withColumn("_dino_table_name", lit("{self.table_name}"))
    .withColumn("_dino_schema_name", lit("{self.target_schema}"))
)

print("‚úÖ Metadados adicionados")
print(f"üìä Registros carregados: {{df_with_metadata.count()}}")

# Mostrar preview dos dados
print("üëÄ Preview dos dados:")
df_with_metadata.select("*").limit(5).show(truncate=False)

{self._generate_write_code()}

print("‚úÖ Ingest√£o batch conclu√≠da com sucesso!")

# Mostrar estat√≠sticas finais
print("üìä Estat√≠sticas da tabela:")
spark.sql(f"SELECT COUNT(*) as total_records FROM {{TARGET_TABLE}}").show()

# Mostrar √∫ltimas ingest√µes
print("üìà √öltimas ingest√µes (√∫ltimas 24h):")
spark.sql(f"""
    SELECT 
        _dino_batch_id,
        _dino_ingestion_timestamp,
        COUNT(*) as records_count,
        _dino_source_path
    FROM {{TARGET_TABLE}}
    WHERE _dino_ingestion_timestamp >= current_timestamp() - INTERVAL 1 DAY
    GROUP BY _dino_batch_id, _dino_ingestion_timestamp, _dino_source_path
    ORDER BY _dino_ingestion_timestamp DESC
    LIMIT 10
""").show(truncate=False)
'''
        return code
    
    def _get_read_options(self) -> Dict[str, str]:
        """Retorna op√ß√µes de leitura baseadas no formato"""
        if self.file_format == "csv":
            return {
                "header": "true",
                "inferSchema": "true", 
                "delimiter": self.delimiter
            }
        elif self.file_format == "json":
            return {
                "multiline": "true"
            }
        else:
            return {}
    
    def _generate_read_code(self) -> str:
        """Gera c√≥digo de leitura baseado no formato"""
        if self.file_format == "csv":
            return f'''df_source = (spark.read
    .format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .option("delimiter", "{self.delimiter}")
    .load(SOURCE_PATH))'''
        
        elif self.file_format == "json":
            return '''df_source = (spark.read
    .format("json")
    .option("multiline", "true")
    .load(SOURCE_PATH))'''
        
        elif self.file_format == "parquet":
            return '''df_source = (spark.read
    .format("parquet")
    .load(SOURCE_PATH))'''
        
        elif self.file_format == "delta":
            return '''df_source = (spark.read
    .format("delta")
    .load(SOURCE_PATH))'''
        
        elif self.file_format == "avro":
            return '''df_source = (spark.read
    .format("avro")
    .load(SOURCE_PATH))'''
        
        else:
            return f'''df_source = (spark.read
    .format("{self.file_format}")
    .load(SOURCE_PATH))'''
    
    def _generate_write_code(self) -> str:
        """Gera c√≥digo de escrita baseado no modo e particionamento"""
        partition_code = ""
        if self.partition_columns:
            partition_cols = '", "'.join(self.partition_columns)
            partition_code = f'.partitionBy("{partition_cols}")'
        
        if self.output_mode == "overwrite":
            return f'''# Salvar com overwrite
print("üíæ Salvando dados (overwrite)...")
(df_with_metadata.write
    .format("delta")
    .mode("overwrite")
    .option("mergeSchema", "true"){partition_code}
    .saveAsTable(TARGET_TABLE))'''
        
        elif self.output_mode == "merge":
            return '''# Implementar merge (upsert) - requer chave prim√°ria
print("üîÑ Implementando merge/upsert...")
# Nota: Para merge, voc√™ precisa definir as colunas de merge
# Exemplo de implementa√ß√£o:
# if DeltaTable.isDeltaTable(spark, TARGET_TABLE):
#     delta_table = DeltaTable.forName(spark, TARGET_TABLE)
#     delta_table.alias("target").merge(
#         df_with_metadata.alias("source"),
#         "target.id = source.id"  # Ajustar conforme sua chave
#     ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
# else:
#     df_with_metadata.write.format("delta").saveAsTable(TARGET_TABLE)

# Por enquanto, usar append
(df_with_metadata.write
    .format("delta")
    .mode("append")
    .option("mergeSchema", "true")
    .saveAsTable(TARGET_TABLE))
print("‚ö†Ô∏è Merge n√£o implementado - usando append")'''
        
        else:  # append
            return f'''# Salvar com append
print("üíæ Salvando dados (append)...")
(df_with_metadata.write
    .format("delta")
    .mode("append")
    .option("mergeSchema", "true"){partition_code}
    .saveAsTable(TARGET_TABLE))'''
    
    def execute_ingestion(self, is_automated: bool = False) -> Dict[str, Any]:
        """
        Executa a ingest√£o (batch ou streaming)
        
        Args:
            is_automated: Se True, gera c√≥digo para streaming com file arrival
                         Se False, gera c√≥digo para ingest√£o batch
        
        Returns:
            Dict com resultado da opera√ß√£o
        """
        try:
            mode = "streaming" if is_automated else "batch"
            print(f"ü¶ï Dino SDK - Iniciando ingest√£o {mode}")
            print(f"üìä Tabela destino: {self.get_table_full_name()}")
            print(f"üìã Formato: {self.file_format}")
            
            # Verificar pr√©-requisitos
            if not self._check_schema_exists():
                raise ValueError(f"Schema {self.target_schema} n√£o existe. Crie o schema antes da ingest√£o.")
            
            # Gerar c√≥digo de ingest√£o baseado no modo
            if is_automated:
                ingestion_code = self._generate_streaming_code()
                filename = f"ingestion_streaming_{self.target_schema}_{self.table_name}.py"
            else:
                ingestion_code = self._generate_batch_code()
                filename = f"ingestion_batch_{self.target_schema}_{self.table_name}.py"
            
            # Salvar c√≥digo em arquivo para execu√ß√£o manual
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(ingestion_code)
            
            print(f"üìù C√≥digo de ingest√£o salvo em: {filename}")
            print(f"üí° Execute este c√≥digo em um notebook Databricks para realizar a ingest√£o")
            
            result = {
                'success': True,
                'table_full_name': self.get_table_full_name(),
                'detected_format': self.file_format,
                'output_mode': self.output_mode,
                'catalog_name': self.catalog_name,
                'schema_name': self.target_schema,
                'table_name': self.table_name,
                'source_path': self.file_path,
                'ingestion_file': filename,
                'is_automated': is_automated,
                'timestamp': datetime.now().isoformat()
            }
            
            # Adicionar checkpoint location se for streaming
            if is_automated:
                result['checkpoint_location'] = self.checkpoint_location
            
            return result
            
        except Exception as e:
            print(f"‚ùå Erro na ingest√£o: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def get_ingestion_status(self) -> Dict[str, Any]:
        """Retorna status da ingest√£o (simula√ß√£o)"""
        # Em ambiente real, consultaria m√©tricas da tabela
        return {
            'status': 'ready_for_execution',
            'table_name': self.get_table_full_name(),
            'last_check': datetime.now().isoformat(),
            'message': 'C√≥digo de ingest√£o gerado. Execute no Databricks.'
        }
